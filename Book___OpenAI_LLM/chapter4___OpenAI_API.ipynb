{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3978542-3c10-4390-8987-cf47d2b8339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# os.getenv('환경변수명')\n",
    "os.environ['OPENAI_API_KEY'] = api_key\n",
    "\n",
    "# openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c188e375-7ff1-4a11-936c-6d17dc83641e",
   "metadata": {},
   "source": [
    "# 텍스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0685a71e-15d3-4d1f-a651-4e454911cdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''다음 이야기를 써주세요.\n",
    "기타를 좋아하지만 컴맹인 여고생이 어떤 계기로 록밴드에 가입하고, 낯선 인간관계를 통해 활동하게 되는 이야기.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca72c313-c00c-42be-abd7-14530f69d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 생성 모델 / 채팅 모델의 경우 구현 방법이 다르다.\n",
    "\n",
    "response = openai.OpenAI().completions.create(\n",
    "                                                model = 'gpt-3.5-turbo-instruct',\n",
    "                                                prompt = prompt,\n",
    "                                                temperature = 0.7,\n",
    "                                                max_tokens = 500\n",
    ")\n",
    "\n",
    "print(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d87554e-86a6-497d-8a1d-12e237b4ccc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "인공지능(AI, Artificial Intelligence)란 인간의 지능을 컴퓨터 프로그램 등으로 구현한 것을 말합니다. 즉, 기계가 인간과 비슷한 지능적인 행동과 결정을 할 수 있는 능력을 갖추는 기술이나 분야를 의미합니다. 인공지능은 주로 머신 러닝, 딥 러닝, 자연어 처리, 컴퓨터 비전 등의 분야를 포함하며, 이를 통해 컴퓨터가 데이터를 분석하고 학습하여 문제를 해결하거나 결정을 내리는 능력을 갖게 됩니다.\n",
      "\n",
      "인공지능은 다양한 분야에서 활용되고 있으며, 많은 기업들이 인공지능 기술을 도입하여 업무를 자동화하고 생산성을 높이는 데 사용하고 있습니다. 예를 들어, 은행에서는 인공지능 기술을 이용해 고객의 대출 신청을 분석하고 적정 대출액을 결정하거나, 인터넷 쇼핑몰에서는 인공지능을 이용하여 고객에게 맞춤형 추천 상품을 제공하는 등 다양한 방식으로 활용되고 있습니다.\n",
      "\n",
      "또한 인공지능은 의료 분야에서도 활용되고 있습니다. 의료 영상을 분석하여 질병을 진단하거나, 환자의 건강 상태를 모니터링하는데 사용됩니다. 또한 인공지능 기술을 이용하여 약물 개발에도 활용되고 있어 새로운 치료법의 개\n"
     ]
    }
   ],
   "source": [
    "prompt = '인공지능에 대해 알려주세요.'\n",
    "\n",
    "response = openai.OpenAI().completions.create(\n",
    "                                                model = 'gpt-3.5-turbo-instruct',\n",
    "                                                prompt = prompt,\n",
    "                                                temperature = 0.7,\n",
    "                                                max_tokens = 500\n",
    ")\n",
    "\n",
    "result = response.choices[0].text\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31e70c06-4d1e-4700-8809-dea77e7f8dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OpenAI는 샘 알트만과 일론 머스크가 설립한 인공지능 연구소로, 친근한 인공지능을 보급하고 발전시키는 것을 목표로 하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 요약\n",
    "prompt = '''아래 문장을 짧은 한 문장으로 요약해 주세요.\n",
    "\n",
    "OpenAI는 영리법인 OpenAI LP와 그 모회사인 비영리법인 OpenAI Inc.로 구성된인공지능 연구소입니다. 20215년 말에 샘 알트만과 일론 머스크 등이 샌프란시스코에서 설립했습니다. 인류 전체에 도움이 되는 방식으로 친근한 인공지능을 보급하고 발전시키는 것을 목표로 삼고 있습니다.\n",
    "'''\n",
    "\n",
    "response = openai.OpenAI().completions.create(\n",
    "                                                model = 'gpt-3.5-turbo-instruct',\n",
    "                                                prompt = prompt,\n",
    "                                                temperature = 0,\n",
    "                                                max_tokens = 500\n",
    ")\n",
    "\n",
    "result = response.choices[0].text\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9418a958-6789-4fa6-aece-3027cfc70b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm a cat.\n"
     ]
    }
   ],
   "source": [
    "# 번역\n",
    "prompt = '''한국어를 영어로 번역해줘.\n",
    "\n",
    "한국어: 나는 고양이다.\n",
    "영어:\n",
    "'''\n",
    "\n",
    "response = openai.OpenAI().completions.create(\n",
    "                                                model = 'gpt-3.5-turbo-instruct',\n",
    "                                                prompt = prompt\n",
    ")\n",
    "\n",
    "result = response.choices[0].text\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "745fc021-713f-448d-80fe-2a0e225eed67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    print('Hello World!')\n",
      "helloworld()\n",
      "\n",
      "#예제2.\n"
     ]
    }
   ],
   "source": [
    "# 코딩\n",
    "prompt = '''#'Hello World!' 표시\n",
    "def helloworld():\n",
    "'''\n",
    "\n",
    "response = openai.OpenAI().completions.create(\n",
    "                                                model = 'gpt-3.5-turbo-instruct',\n",
    "                                                prompt = prompt\n",
    ")\n",
    "\n",
    "result = response.choices[0].text\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "177d1106-ea39-42fc-a674-8f4ad6d57b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕! 무슨 일 있니?\n"
     ]
    }
   ],
   "source": [
    "# 채팅\n",
    "'''\n",
    "role(역할)\n",
    "- system : 채팅 AI의 행동에 대한 지시\n",
    "- user : 인간의 발화\n",
    "- assistant : AI의 발언\n",
    "\n",
    "content(콘텐츠)\n",
    "'''\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': '아카네는 여고생 여동생 캐리턱의 채팅 AI입니다. 남동생과 대화합니다.'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': '안녕?'\n",
    "    }\n",
    "]\n",
    "\n",
    "response = openai.OpenAI().chat.completions.create(\n",
    "                                                    model = 'gpt-3.5-turbo-1106',\n",
    "                                                    messages = messages,\n",
    "                                                    temperature = 0 # 무작위성. 창의적으로 만들려면 0.8, 답이 있는 경우 0 권장\n",
    ")\n",
    "\n",
    "result = response.choices[0].message.content\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "92ab2ca6-8251-4640-b778-6687ca83dd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 삽입 - 이건 무슨 경우에 쓰는거지?\n",
    "prefix_prompt = \"\"\"def helloworld():\n",
    "'''\n",
    "설명:\n",
    "\"\"\"\n",
    "\n",
    "suffix_prompt = \"\"\"\n",
    "'''\n",
    "    print(\"Hello World!\")\n",
    "\n",
    "helloworld()\n",
    "\"\"\"\n",
    "\n",
    "response = openai.OpenAI().completions.create(\n",
    "                                                model = 'gpt-3.5-turbo-instruct',\n",
    "                                                prompt = prefix_prompt,\n",
    "                                                suffix = suffix_prompt,\n",
    "                                                temperature = 0.7,\n",
    "                                                max_tokens = 300\n",
    ")\n",
    "\n",
    "result = response.choices[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "882bd65c-6c92-47e6-ab88-531c58ce6470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "helloworld() 함수는 Hello World를 출력합니다.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294feaef-48c8-4805-99c9-87e9ea75abb5",
   "metadata": {},
   "source": [
    "# 이미지 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "01edbe2d-86b9-4853-b579-cc80395537ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://oaidalleapiprodscus.blob.core.windows.net/private/org-1UTAxR1umHeQseIG9pTmTH1C/user-ppNmDLRzAv6kYSCsugbVxaQA/img-ZUbfBUc4hbwllkPLewAeoJ36.png?st=2024-01-18T00%3A59%3A05Z&se=2024-01-18T02%3A59%3A05Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-01-17T14%3A40%3A30Z&ske=2024-01-18T14%3A40%3A30Z&sks=b&skv=2021-08-06&sig=JRqbBtCHXduMrHqqYvxTCsDzkAoYZHT6tcT%2B0mebguU%3D\n"
     ]
    }
   ],
   "source": [
    "prompt = 'cat dancing on a car'\n",
    "\n",
    "response = openai.OpenAI().images.generate(prompt = prompt,\n",
    "                                           n = 1,\n",
    "                                           size = '512x512')\n",
    "image_url = response.data[0].url\n",
    "print(image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b2640d47-101a-4c2d-99d4-ba21c8c25cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = open('image.png', 'rb')\n",
    "mask = open('mask.png', 'rb')\n",
    "\n",
    "prompt = 'many aplles in carboard box'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "31aad8ab-47bb-4d0c-8ce7-1fd67c6736a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://oaidalleapiprodscus.blob.core.windows.net/private/org-1UTAxR1umHeQseIG9pTmTH1C/user-ppNmDLRzAv6kYSCsugbVxaQA/img-O1D3wDOr0PkCBsuAW1RjHoTE.png?st=2024-01-18T01%3A38%3A53Z&se=2024-01-18T03%3A38%3A53Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-01-17T12%3A28%3A31Z&ske=2024-01-18T12%3A28%3A31Z&sks=b&skv=2021-08-06&sig=pJTcuMNPjjn8fmffRKawChKVkCo5RpSa4w%2BpYdU1Sio%3D\n"
     ]
    }
   ],
   "source": [
    "response = openai.OpenAI().images.edit(\n",
    "                                        image = image,\n",
    "                                        mask = mask,\n",
    "                                        prompt = prompt,\n",
    "                                        n = 1,\n",
    "                                        size = '512x512'\n",
    ")\n",
    "image_url = response.data[0].url\n",
    "print(image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "945dd2b1-94af-43d7-ba6c-a5fc5b9dcf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://oaidalleapiprodscus.blob.core.windows.net/private/org-1UTAxR1umHeQseIG9pTmTH1C/user-ppNmDLRzAv6kYSCsugbVxaQA/img-IFNKv0PAy9DFLHsArlfciWJx.png?st=2024-01-18T01%3A41%3A32Z&se=2024-01-18T03%3A41%3A32Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-01-17T12%3A18%3A31Z&ske=2024-01-18T12%3A18%3A31Z&sks=b&skv=2021-08-06&sig=g8TaPZauYzXephO%2BnbdEsm9zhJpAqqVayOZbChCtEnA%3D\n"
     ]
    }
   ],
   "source": [
    "response = openai.OpenAI().images.create_variation(\n",
    "                                                    image = image,\n",
    "                                                    n = 1,\n",
    "                                                    size = '512x512'\n",
    ")\n",
    "image_url = response.data[0].url\n",
    "print(image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5c4fb6-11ef-48e5-868d-25fd2b562b46",
   "metadata": {},
   "source": [
    "# 임베딩\n",
    "\n",
    "텍스트를 벡터로 변환하는 과정을 임베딩이라고 칭한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "65c1afb2-a00f-446b-8305-0c8e0981da08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5f13cd71-8df0-4da9-84ad-6b95acfb1b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "text = '이것은 테스트입니다.'\n",
    "model = 'text-embedding-ada-002' # 1536 차원으로 임베딩 진행\n",
    "\n",
    "response = client.embeddings.create(input = [text],\n",
    "                                    model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "efcdc017-28f5-44c3-9c03-0d34980c5ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(response.data[0].embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "21c6b820-d7fb-47a9-9280-5ba557214e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "text1 = '이것은 테스트입니다.'\n",
    "text2 = '이것은 실전입니다.'\n",
    "model = 'text-embedding-ada-002' # 해당 모델이 생성하는 임베딩의 차원은 1536, 최대 토큰수는 8191\n",
    "\n",
    "response = client.embeddings.create(input = [text1, text2],\n",
    "                                    model = model)\n",
    "\n",
    "# response.data <- list의 형태로, input 값에 대한 임베딩 배열이 리스트 원소에 들어있다.\n",
    "# response.data[0].embedding # 1536 차원 벡터로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7752de9a-5c9c-4af7-963f-d88701eb80d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n",
      "1536\n"
     ]
    }
   ],
   "source": [
    "print(len(response.data[0].embedding)) # 이것은 테스트입니다. 의 임베딩 벡터 값\n",
    "print(len(response.data[1].embedding)) # 이것은 실전입니다. 의 임베딩 벡터 값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a01039f-40f5-4cf5-867f-8a2bd0fb5f48",
   "metadata": {},
   "source": [
    "### 유사도 검색\n",
    "- 코사인 유사도\n",
    "- L2노름 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d4a313d2-ee10-4fff-8b84-095c6724edfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from numpy.linalg import norm # numpy 패키지를 통한 유사도 계산\n",
    "\n",
    "import faiss # faiss 패키지를 통한 유사도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7bd0a22c-7ce2-4ee2-b5e7-21e942e48e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "in_text = '오늘은 비가 오지 않아서 다행이다.'\n",
    "model = 'text-embedding-ada-002'\n",
    "\n",
    "response = client.embeddings.create(input = [in_text],\n",
    "                                    model = model)\n",
    "\n",
    "in_embeds = [record.embedding for record in response.data]\n",
    "in_embeds = np.array(in_embeds).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "159caf2b-189e-4494-9f61-733b1bed15f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1536)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "20249bea-d112-4e2a-bc34-d42f4861a9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_texts = [\n",
    "                '좋아하는 음식은 무엇인가요?',\n",
    "                '어디에 살고 계신가요?',\n",
    "                '아침 전철은 혼잡하네요',\n",
    "                '오늘은 날씨가 좋네요!',\n",
    "                '요즘 경기가 좋지 않네요.'\n",
    "]\n",
    "\n",
    "response = client.embeddings.create(input = target_texts,\n",
    "                                    model = model)\n",
    "\n",
    "target_embeds = [record.embedding for record in response.data]\n",
    "target_embeds = np.array(target_embeds).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "20d42ab3-68ac-4342-89a2-1be8455a46a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x7f0f0a3becc0> >"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faiss.IndexFlatL2(1536)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "feda2a60-f31d-4c6f-ac9a-8b88b43ebee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IndexFlatL2(L2 노름)\n",
    "# IndexFlatIP(코사인 유사도)\n",
    "# IndexIVFFlat(고속화 알고리즘)\n",
    "\n",
    "index = faiss.IndexFlatL2(1536)\n",
    "index = faiss.IndexFlatIP(1536)\n",
    "\n",
    "n_list = 2 # 임베딩 차원수보다 작아야 한다.\n",
    "quantizer = faiss.IndexFlatL2(1536)\n",
    "index = faiss.IndexIVFFlat(quantizer, 1536, n_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "037642f9-284b-4620-bddc-239d0d4f1eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 5 points to 2 centroids: please provide at least 78 training points\n"
     ]
    }
   ],
   "source": [
    "# index.add(target_embeds)\n",
    "index.train(target_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5505af04-bda4-41d6-913f-724a5d9152e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33448073]]\n",
      "[[3]]\n",
      "오늘은 날씨가 좋네요!\n"
     ]
    }
   ],
   "source": [
    "D, I = index.search(in_embeds, 1)\n",
    "\n",
    "print(D) # distance\n",
    "print(I) # index\n",
    "print(target_texts[I[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2dc420f5-643e-4a32-8f07-9ed4cc4c9aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "벡터1과 벡터2의 유사도: 0.67\n",
      "벡터1과 벡터3의 유사도: 0.67\n",
      "벡터2와 벡터3의 유사도: 1.00\n"
     ]
    }
   ],
   "source": [
    "# numpy를 통한 코사인 유사도 계산\n",
    "def cos_sim(A, B):\n",
    "    result = np.dot(A, B) / (norm(A) * norm(B))\n",
    "    \n",
    "    return result\n",
    "\n",
    "vec1 = np.array([0, 1, 1, 1])\n",
    "vec2 = np.array([1, 0, 1, 1])\n",
    "vec3 = np.array([2, 0, 2, 2])\n",
    "\n",
    "print(f'벡터1과 벡터2의 유사도: {cos_sim(vec1, vec2):.2f}')\n",
    "print(f'벡터1과 벡터3의 유사도: {cos_sim(vec1, vec3):.2f}')\n",
    "print(f'벡터2와 벡터3의 유사도: {cos_sim(vec2, vec3):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a77239f-9f3f-42a1-be98-3446b330df5f",
   "metadata": {},
   "source": [
    "# 파인튜닝\n",
    "\n",
    "사전 학습된 모델을 기반으로 개별 작업에 맞게 추가 학습을 하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "96223011-5621-4765-8081-c3787c2ca7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c359b046-120f-4341-9c94-77cf35780f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./tsukuyomi.csv',\n",
    "                 usecols = [1, 2],\n",
    "                 names = ['prompt', 'completion'],\n",
    "                 skiprows = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b7b4c441-7995-422c-b42a-2d4ba5f567a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 프레임을 json형태로 변형(학습 데이터에서 요구하는 포맷)\n",
    "df.to_json('./tsukuyomi.jsonl',\n",
    "           orient = 'records',\n",
    "           lines = True,\n",
    "           force_ascii = False)\n",
    "\n",
    "'''\n",
    "    {'prompt': '프롬프트 예시1', 'completion': '컴플리션 예시1'}\n",
    "    {'prompt': '프롬프트 예시2', 'completion': '컴플리션 예시2'}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "94ca8edc-bfa8-4d75-b7dd-615e92d58232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat(row):\n",
    "    result = json.dumps(\n",
    "                        {'messages': [\n",
    "                                        {'role': 'system', 'content': '츠쿠요미는 여성 캐릭터 입니다.'},\n",
    "                                        {'role': 'user', 'content': row['prompt']},\n",
    "                                        {'role': 'assistant', 'content': row['completion']}\n",
    "                        ]}\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "    \n",
    "df['conversation'] = df.apply(format_chat, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0704ff75-6ff0-4474-a527-fb9b8df68ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./tsukuyomi_prepared.jsonl', 'w') as jsonl_file:\n",
    "    for row in df['conversation']:\n",
    "        jsonl_file.write(row + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761c884c-e020-47e1-9177-865f55d5f48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./tsukuyomi_prepared.jsonl', 'r') as f:\n",
    "#     test_file = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2d39e7e8-0057-4718-82d7-bec3abbb024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a9a8e6e5-e02b-43c3-89e1-98183552180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = client.files.create(file = open('./tsukuyomi_prepared.jsonl', 'rb'),\n",
    "                           purpose = 'fine-tune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1597e942-809a-4ab1-bf76-0553837ec8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='file-3UrWgFhnkmz67e6PpZNjhUMj', bytes=184146, created_at=1701829522, filename='tsukuyomi_prepared.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)\n"
     ]
    }
   ],
   "source": [
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1c40ee6f-b2bd-4b0e-ae0a-f815f229fee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-3UrWgFhnkmz67e6PpZNjhUMj', bytes=184146, created_at=1701829522, filename='tsukuyomi_prepared.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.files.retrieve(file.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "da9f82ce-1606-4914-ae86-baec63b6b1e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'Fine-tuning jobs cannot be created on an Explore plan. You can upgrade to a paid plan on your billing page: https://platform.openai.com/account/billing/overview', 'type': 'invalid_request_error', 'param': None, 'code': 'exceeded_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfine_tuning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/openai/resources/fine_tuning/jobs.py:104\u001b[0m, in \u001b[0;36mJobs.create\u001b[0;34m(self, model, training_file, hyperparameters, suffix, validation_file, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m     51\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FineTuningJob:\n\u001b[1;32m     52\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    Creates a job that fine-tunes a specified model from a given dataset.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/fine_tuning/jobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_file\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhyperparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msuffix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation_file\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjob_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJobCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFineTuningJob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/openai/_base_client.py:1055\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1043\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1051\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1052\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1053\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1054\u001b[0m     )\n\u001b[0;32m-> 1055\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/openai/_base_client.py:834\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    827\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    832\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    833\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/openai/_base_client.py:877\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    875\u001b[0m     \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 877\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'Fine-tuning jobs cannot be created on an Explore plan. You can upgrade to a paid plan on your billing page: https://platform.openai.com/account/billing/overview', 'type': 'invalid_request_error', 'param': None, 'code': 'exceeded_quota'}}"
     ]
    }
   ],
   "source": [
    "client.fine_tuning.jobs.create(training_file = file.id,\n",
    "                               model = 'gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d998e8d9-442f-4b12-a6f1-1b6572f0bdc7",
   "metadata": {},
   "source": [
    "# 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "88f3f16e-b1f4-44d0-bd8a-7e9228a4cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "85324d5a-b22c-4d99-9509-a0f1e6f12d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding('cl100k_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6521c360-ca5d-4e43-a328-8480d841e1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9906, 4435, 0]\n",
      "Hello World!\n",
      "[b'Hello', b' World', b'!']\n"
     ]
    }
   ],
   "source": [
    "tokens = enc.encode('Hello World!')\n",
    "print(tokens)\n",
    "print(enc.decode(tokens))\n",
    "print(enc.decode_tokens_bytes(tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
