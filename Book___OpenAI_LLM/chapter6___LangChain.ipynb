{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f02dc789-6a9c-46f0-9330-f4a234a110b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d4075d5-7380-44a4-ac34-1a92b75e287b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "\"스빌라스\"\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(temperature = 0.9)\n",
    "print(llm('컴퓨터 게임을 만드는 새로운 한국어 회사명을 하나 제안해 주세요'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b7b16b8-d3dc-4d1c-b0be-56255d06b9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가정용 로봇을 만드는 새로운 한국어 회사명을 하나 제안해 주세요.\n"
     ]
    }
   ],
   "source": [
    "# prompt template\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "                        input_variables = ['product'],\n",
    "                        template = '{product}을 만드는 새로운 한국어 회사명을 하나 제안해 주세요.'\n",
    ")\n",
    "\n",
    "print(prompt.format(product = '가정용 로봇'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78c04ef6-5456-4d3c-927a-2a7c96e8f150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\n\\n우리집로봇 (UrijibRobot)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c# prompt template\n",
    "prompt = PromptTemplate(c\n",
    "                        input_variables = ['product'],\n",
    "                        template = '{product}을 만드는 새로운 한국어 회사명을 하나 제안해주세요'\n",
    ")\n",
    "\n",
    "# chain\n",
    "chain = LLMChain(\n",
    "                    llm = OpenAI(temperature = 0.9),\n",
    "                    prompt = prompt\n",
    ")\n",
    "\n",
    "# run\n",
    "chain.run('가정용 로봇')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed3b405f-40a5-407f-86ae-4a49d3b3d3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chain = ConversationChain(\n",
    "                            llm = OpenAI(temperature = 0),\n",
    "                            verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd08805c-4e43-466e-951d-13e27dd4ce25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 우리집 반려견 이름은 보리입니다.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 반갑습니다, 보리씨! 보리는 어떤 견종인가요?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run('우리집 반려견 이름은 보리입니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "803dfc2e-fc72-4690-83db-d329e6313970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 우리집 반려견 이름은 보리입니다.\n",
      "AI:  반갑습니다, 보리씨! 보리는 어떤 견종인가요?\n",
      "Human: 우리집 반려견 이름을 불러주세요\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 보리를 불러주겠습니다! \"보리, 여기 와라!\"'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(input = '우리집 반려견 이름을 불러주세요')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4db7546-3931-4605-9356-a2625db469e9",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8edaafa1-4ddc-4aeb-862e-c048ec59986e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    model_name : OpenAI API의 모델명\\n    max_tokens : 최대 출력 토큰수\\n    temperature : 무작위성\\n    n : 생성할 결과 수\\n    cache : 캐시 활성화/비활성화 선택\\n    streaming : 스트리밍 활성화/비활성화 선택\\n    callback_manager : 콜백 매니저\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(\n",
    "                model_name = 'text-davinci-003',\n",
    "                temperature = 0\n",
    ")\n",
    "\n",
    "# 주요매개변수\n",
    "'''\n",
    "    model_name : OpenAI API의 모델명\n",
    "    max_tokens : 최대 출력 토큰수\n",
    "    temperature : 무작위성\n",
    "    n : 생성할 결과 수\n",
    "    cache : 캐시 활성화/비활성화 선택\n",
    "    streaming : 스트리밍 활성화/비활성화 선택\n",
    "    callback_manager : 콜백 매니저\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc9da128-2967-4318-9cc7-96408b78b9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "고양이 울음소리는 \"야옹\"으로 나타납니다.\n"
     ]
    }
   ],
   "source": [
    "result = llm('고양이 울음소리는?')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc0a1556-4c8a-4f60-ad74-3165ce8aac1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: \n",
      "\n",
      "고양이 울음소리는 \"야옹\"으로 나타납니다.\n",
      "response: \n",
      "\n",
      "까마귀의 울음소리는 \"까악까악\"이라고 합니다.\n",
      "\n",
      "llm_output: {'token_usage': {'completion_tokens': 109, 'prompt_tokens': 47, 'total_tokens': 156}, 'model_name': 'text-davinci-003'}\n"
     ]
    }
   ],
   "source": [
    "# 고급 llm 호출\n",
    "result = llm.generate(['고양이 울음소리는?', '까마귀 울음소리는?'])\n",
    "\n",
    "print('response:', result.generations[0][0].text)\n",
    "print('response:', result.generations[1][0].text)\n",
    "\n",
    "print('\\nllm_output:', result.llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3a7c005-aa89-4c42-ba26-f70a97da79ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "chat_llm = ChatOpenAI(\n",
    "                        model_name = 'gpt-3.5-turbo',\n",
    "                        temperature = 0\n",
    ")\n",
    "\n",
    "messages = [HumanMessage(content = '고양이 울음소리는?')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2c247e3-85d6-4289-82ea-c0cb09cda147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='고양이의 울음소리는 \"야옹\"이라고 표현됩니다.'\n"
     ]
    }
   ],
   "source": [
    "result = chat_llm(messages)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d39a3ea-31fd-492e-91e2-7feee54a8929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "고양이의 울음소리는 \"야옹\"이라고 표현됩니다.\n",
      "까악까악\n",
      "{'token_usage': {'completion_tokens': 34, 'prompt_tokens': 38, 'total_tokens': 72}, 'model_name': 'gpt-3.5-turbo'}\n"
     ]
    }
   ],
   "source": [
    "messages_list = [\n",
    "    [HumanMessage(content = '고양이 울음소리는?')],\n",
    "    [HumanMessage(content = '까마귀 울음소리는?')]\n",
    "]\n",
    "\n",
    "result = chat_llm.generate(messages_list)\n",
    "\n",
    "print(result.generations[0][0].text)\n",
    "print(result.generations[1][0].text)\n",
    "\n",
    "print(result.llm_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa7472-cb54-4208-a97d-080f7b15fe88",
   "metadata": {},
   "source": [
    "### Cache(캐시)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e6241c1-4dec-41c2-8f20-03c3c238cd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.cache import InMemoryCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "302d164e-a484-4e82-92db-d0d5d954336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.llm_cache = InMemoryCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c40c550-8009-4f44-9b0b-46fc77f59192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='\\n\\n하늘의 색깔은 파랑색입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'prompt_tokens': 16, 'total_tokens': 51, 'completion_tokens': 35}, 'model_name': 'text-davinci-003'}, run=[RunInfo(run_id=UUID('92ac33ab-aa9b-461f-87fa-1bfdcfa255c3'))])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.generate(['하늘의 색깔은?']) # llm_output 정보가 있는 것을 통해 API를 호출하여 결과를 작성했음을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f23cbd89-f4f7-4509-8067-e5065a5a7387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='\\n\\n하늘의 색깔은 파랑색입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={}, run=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.generate(['하늘의 색깔은?']) # llm_output이 비어있다 >>> API 호출 없이 캐시에서 결과를 읽어온 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9ba773d-7d6c-4509-a5ba-c06380c8dfc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='\\n\\n하늘의 색깔은 파랑입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'prompt_tokens': 16, 'total_tokens': 48, 'completion_tokens': 32}, 'model_name': 'text-davinci-003'}, run=[RunInfo(run_id=UUID('3b25a5cd-252e-4fcc-8627-998a79af7871'))])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(cache = False)\n",
    "\n",
    "llm.generate(['하늘의 색깔은?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33638b83-9a18-42b2-999e-99e459c5cef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='\\n\\n하늘의 색깔은 밝은 파란색이라고 할 수 있습니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'prompt_tokens': 16, 'total_tokens': 73, 'completion_tokens': 57}, 'model_name': 'text-davinci-003'}, run=[RunInfo(run_id=UUID('1587f8f8-1773-4b72-968d-44607a024d95'))])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전역 캐시 비활성화\n",
    "langchain.llm_cache = None\n",
    "\n",
    "llm.generate(['하늘의 색깔은?'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cfb174-91f0-4be0-b216-6e382082d4d6",
   "metadata": {},
   "source": [
    "### 비동기 처리\n",
    "- 동기식 처리: 프로그램이 하나의 작업을 완료할 때까지 다른 작업을 시작하지 않는 방식\n",
    "- 비동기식 처리: 프로긂이 작업을 시작함과 동시에 여러 작업을 같이 시작하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05cb9855-a9a3-49e4-adaa-6710f2bf9222",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 동기화 처리시에 소요되는 시간 확인\n",
    "# import time\n",
    "\n",
    "# def generate_serially():\n",
    "#     llm = OpenAI(temperature = 0.9)\n",
    "    \n",
    "#     for _ in range(10):\n",
    "#         resp = llm.generate(['안녕하세요'])\n",
    "#         print(resp.generations[0][0].text)\n",
    "        \n",
    "\n",
    "# s = time.perf_counter()\n",
    "\n",
    "# generate_serially()\n",
    "\n",
    "# elasped = time.perf_counter() - s\n",
    "# print(f'{elasped:.2f}초')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "443f7c24-2e4c-400f-88c3-202e175f5ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 비동기화 처리\n",
    "# import asyncio\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# async def async_generate(llm):\n",
    "#     resp = await llm.agenerate(['안녕하세요!'])\n",
    "#     print(resp.generations[0][0].text)\n",
    "    \n",
    "# async def generate_concurrently():\n",
    "#     llm = OpenAI(temperature = 0.9)\n",
    "#     tasks = [async_generate(llm) for _ in range(10)]\n",
    "#     await asyncio.gather(*task)\n",
    "    \n",
    "# s = time.perf_counter()\n",
    "\n",
    "# asyncio.run(generate_concurrently())\n",
    "\n",
    "# elasped = time.perf_counter() - s\n",
    "# print(f'{elasped:.2f}초')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d88642c-59d2-43ad-ac14-4a7ca1c02e06",
   "metadata": {},
   "source": [
    "### 스트리밍\n",
    "\n",
    "LLM의 스트리밍은 한 번에 모두 출려하지 않고, 토큰 단위로 출력을 돌려줌으로써 체감하는 대기 시간을 줄이는 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b0c938-5210-41e4-a53d-5481ed22e845",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for text-davinci-003 in organization org-3TYHE6vbWw0Dszy6PlAq2x2O on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreaming_stdout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StreamingStdOutCallbackHandler\n\u001b[1;32m      3\u001b[0m llm \u001b[38;5;241m=\u001b[39m OpenAI(\n\u001b[1;32m      4\u001b[0m                 streaming \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m                 callbacks \u001b[38;5;241m=\u001b[39m [StreamingStdOutCallbackHandler()],\n\u001b[1;32m      6\u001b[0m                 verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m                 temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m즐거운 ChatGPT 생활을 가사로 만들어 주세요.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/langchain/llms/base.py:876\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    873\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    874\u001b[0m     )\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    886\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/langchain/llms/base.py:656\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    642\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m         )\n\u001b[1;32m    644\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    646\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    654\u001b[0m         )\n\u001b[1;32m    655\u001b[0m     ]\n\u001b[0;32m--> 656\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/langchain/llms/base.py:544\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    543\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    545\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/langchain/llms/base.py:531\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    523\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    528\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    530\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 531\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    535\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    539\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    540\u001b[0m         )\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    542\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/langchain/llms/openai.py:436\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot stream results with multiple prompts.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    435\u001b[0m generation: Optional[GenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 436\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(_prompts[\u001b[38;5;241m0\u001b[39m], stop, run_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m         generation \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/langchain/llms/openai.py:356\u001b[0m, in \u001b[0;36mBaseOpenAI._stream\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invocation_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_sub_prompts(params, [prompt], stop)  \u001b[38;5;66;03m# this mutates params\u001b[39;00m\n\u001b[0;32m--> 356\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_resp, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    360\u001b[0m         stream_resp \u001b[38;5;241m=\u001b[39m stream_resp\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/langchain/llms/openai.py:114\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[0;34m(llm, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(llm, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/openai/_utils/_utils.py:299\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/openai/resources/completions.py:559\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    557\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    558\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Completion \u001b[38;5;241m|\u001b[39m Stream[Completion]:\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_of\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mecho\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msuffix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCompletion\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/openai/_base_client.py:1055\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1043\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1051\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1052\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1053\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1054\u001b[0m     )\n\u001b[0;32m-> 1055\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/openai/_base_client.py:834\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    827\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    832\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    833\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/openai/_base_client.py:865\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m--> 865\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    875\u001b[0m     \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/openai/_base_client.py:925\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    923\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/openai/_base_client.py:865\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m--> 865\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    875\u001b[0m     \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/openai/_base_client.py:925\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    923\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/openai/_base_client.py:877\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    875\u001b[0m     \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 877\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for text-davinci-003 in organization org-3TYHE6vbWw0Dszy6PlAq2x2O on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "llm = OpenAI(\n",
    "                streaming = True,\n",
    "                callbacks = [StreamingStdOutCallbackHandler()],\n",
    "                verbose = True,\n",
    "                temperature = 0\n",
    ")\n",
    "\n",
    "resp = llm('즐거운 ChatGPT 생활을 가사로 만들어 주세요.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4167b2b-681f-467a-acdf-d40485edf12e",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f63837-36cc-4bdf-9ee6-608221e83391",
   "metadata": {
    "tags": []
   },
   "source": [
    "### prompt가 뭐야?\n",
    "https://www.thedatahunt.com/trend-insight/what-is-prompt\n",
    "\n",
    "프롬프트 엔지니어링, Prompt Engineering 이란?\n",
    "최근 몇 년 동안 자연어 처리(NLP)의 발전으로 입력 프롬프트를 기반으로 사람과 유사한 텍스트를 생성할 수 있는 ChatGPT 및 GPT-4와 같은 강력한 언어 모델이 개발되었습니다. 생성형 AI가 사람의 말, 자연어를 알아들을 수 있게 되었지만 여전히 컴퓨터의 특성에 맞춰서 AI에게 input해야 하는 한계는 남아 있습니다. 프롬프트의 품질에 따라서 의도는 같지만 다르게 말하기 때문에 AI의 답변 내용을 달라지게 됩니다.\n",
    "\n",
    "이것이 바로 프롬프트 엔지니어링이 필요한 이유입니다. 프롬프트를 신중하게 제작하고 최적화함으로써 LLM을 사용하여 보다 정확하고 관련성이 높으며 매력적인 텍스트를 생성할 수 있습니다. 데이터 사이언티스트, 콘텐츠 제작자 또는 자연어 처리의 기능을 탐색하는 데 관심이 있는 사람이라면 프롬프트 엔지니어링을 사용하는 방법을 배우면 목표를 달성하는 데 도움이 될 수 있습니다.\n",
    "\n",
    "‍\n",
    "\n",
    "예를 들면, 아래 처럼 2가지 다른 질문 방법이 있다면 2번째 질문이 더 이해하기 쉬운 답변을 주게 됩니다.\n",
    "\n",
    "‍\n",
    "\n",
    "첫번째 질문 유형\n",
    "블랙홀에 대해서 설명해줘.\n",
    "두번째 질문 유형\n",
    "다음은 AI 연구 보조원과의 대화입니다.\n",
    "보조원은 초등학생도 이해할 수 있는 수준으로 답변해야 합니다.\n",
    "질문: 블랙홀 생성에 대해 설명해줄래?\n",
    "AI:\n",
    "‍\n",
    "\n",
    "이렇게 자연어 명령의 의도를 잘 파악하고 더 좋은 답변을 주기 위한 프롬프트의 구성을 만들어 가는 것을 프롬프트 엔지니어링, Prompt Engineering 이라고 합니다. 프롬프트 엔지니어링은 인공지능 분야의 한 개념으로 AI로부터 높은 수준의 결과물을 얻기 위해 적절한 프롬프트를 구성하는 작업입니다.\n",
    "\n",
    "아무리 고도화된 Generative AI라고 해도, 인간의 의도와 목적을 완벽하게 파악할 수 있는 것은 아닙니다. 때로는 부적절하거나 오류가 있는 결과물을 생성하기도 하고, 인간의 창의성이나 감성을 완전히 대체할 수는 없습니다. 그렇기 때문에 생성형 AI를 제대로 활용하기 위해서는 인간의 감독과 조정이 필요합니다. 최근에는 프롬프트 엔지니어라는 새로운 직군이 나타났으며 그들의 가치도 급상승하고 있습니다. AI를 이용하기 위해 프롬프트를 탐색하고 설계하여 개발하는 과정이 그 만큼 중요해진 시대 상황을 잘 보여주고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2c8cf232-71ee-4850-853f-4d3a1725abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ccae3575-a473-4e40-8e83-b6ec633708f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "멋진 동물이라고 하면?\n"
     ]
    }
   ],
   "source": [
    "no_input_prompt = PromptTemplate(input_variables = [],\n",
    "                                 template = '멋진 동물이라고 하면?')\n",
    "\n",
    "print(no_input_prompt.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5ba3505b-3530-4e02-b0f8-3d9c078c4dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "멋진 동물이라고 하면?\n"
     ]
    }
   ],
   "source": [
    "# 하나의 입력 변수가 있는 프롬프트 템플릿\n",
    "one_input_prompt = PromptTemplate(input_variables = ['content'],\n",
    "                                  template = '멋진 {content}이라고 하면?')\n",
    "\n",
    "print(one_input_prompt.format(content = '동물'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0321f091-64e0-4a9f-8b91-28369df76bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "멋진 동물이라고 하면?\n"
     ]
    }
   ],
   "source": [
    "# 여러개의 입력 변수가 있는 프롬프트 템플릿\n",
    "multiple_inpurt_prompt = PromptTemplate(input_variables = ['adjective', 'content'],\n",
    "                                        template = '{adjective} {content}이라고 하면?')\n",
    "\n",
    "print(multiple_inpurt_prompt.format(adjective = '멋진', content = '동물'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1166550c-4b4e-425b-9b3e-ac7a37a8c60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                \n",
      "                                Q: foo\n",
      "                                A: bar\n",
      "                                \n",
      "                                Q: \n",
      "                                A: 2\n",
      "                                \n",
      "                                \n"
     ]
    }
   ],
   "source": [
    "jinja2_prompt = PromptTemplate(\n",
    "                                input_variables = ['items'], # format 메서드에 입력할 변수 값 리스트\n",
    "                                template_format = 'jinja2',\n",
    "                                template = '''\n",
    "                                {% for item in items %}\n",
    "                                Q: {{item.question}}\n",
    "                                A: {{item.answer}}\n",
    "                                {% endfor %}\n",
    "                                '''\n",
    ")\n",
    "\n",
    "items = [\n",
    "            {'question': 'foo', 'answer': 'bar'},\n",
    "            {'qeustion': '1', 'answer': '2'}\n",
    "]\n",
    "\n",
    "print(jinja2_prompt.format(items = items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a63de5a9-f86f-490d-94ff-06463866f642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 입력에 대한 반의어를 입력하세요\n",
      "\n",
      "입력: 밝은\n",
      "출력: 어두운\n",
      "\n",
      "입력: 재미있는\n",
      "출력: 지루한\n",
      "\n",
      "입력: 큰\n",
      "출력:\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {'input': '밝은', 'output': '어두운'},\n",
    "    {'input': '재미있는', 'output': '지루한'}\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "                                input_variables = ['input', 'ouptut'],\n",
    "                                template = '입력: {input}\\n출력: {output}'\n",
    ")\n",
    "\n",
    "prompt_from_string_example = FewShotPromptTemplate(\n",
    "                                                    examples = examples, # 답변에 대한 예시\n",
    "                                                    example_prompt = example_prompt,\n",
    "                                                    prefix = '모든 입력에 대한 반의어를 입력하세요', # 접두사\n",
    "                                                    suffix = '입력: {adjective}\\n출력:', # 접미사\n",
    "                                                    input_variables = ['adjective'],\n",
    "                                                    example_separator = '\\n\\n'\n",
    ")\n",
    "\n",
    "# 접두사 - 예시 - 접미사 의 형태로 결과물 출력\n",
    "print(prompt_from_string_example.format(adjective = '큰'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a5c132-7b20-4fd5-9093-54cb25665778",
   "metadata": {},
   "source": [
    "### 다양한 답변 예시의 포함\n",
    "- LengthBasedExampleSelector : 문자열 길이를 기준으로 답변 선택\n",
    "- SemanticSimilarityExampleSelector : 입력과 가장 유사한 답변 예제를 기준으로 선택(코사인 유사도)\n",
    "- MaxMarginalRelevanceExampleSelector : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa57ac8e-b297-4c79-9471-39d97c900053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 입력에 대한 반의어를 입력하세요\n",
      "\n",
      "입력: 밝은\n",
      "출력: 어두운\n",
      "\n",
      "입력: 재미있는\n",
      "출력: 지루한\n",
      "\n",
      "입력: 큰\n",
      "출력:\n"
     ]
    }
   ],
   "source": [
    "# LengthBasedExampleSelector\n",
    "from langchain.prompts import FewShotPromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "examples = [\n",
    "            {'input': '밝은', 'output': '어두운'},\n",
    "            {'input': '재미있는', 'output': '지루한'},\n",
    "            {'input': '활기찬', 'output': '무기력한'},\n",
    "            {'input': '높은', 'output': '낮은'},\n",
    "            {'input': '빠른', 'output': '느린'}\n",
    "]\n",
    "\n",
    "# 프롬프트 템플릿 생성\n",
    "example_prompt = PromptTemplate(\n",
    "                                input_variables = ['input', 'output'],\n",
    "                                template = '입력: {input}\\n출력: {output}'\n",
    ")\n",
    "\n",
    "# 답변 선택기\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "                                                examples = examples,\n",
    "                                                example_prompt = example_prompt,\n",
    "                                                max_length = 10 # 문자열의 최대 길이\n",
    ")\n",
    "\n",
    "prompt_from_string_examples = FewShotPromptTemplate(\n",
    "                                                    example_selector = example_selector,\n",
    "                                                    example_prompt = example_prompt,\n",
    "                                                    prefix = '모든 입력에 대한 반의어를 입력하세요',\n",
    "                                                    suffix = '입력: {adjective}\\n출력:',\n",
    "                                                    input_variables = ['adjective'],\n",
    "                                                    # example_seperator = '\\n\\n'\n",
    ")\n",
    "\n",
    "print(prompt_from_string_examples.format(adjective = '큰'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0b33dbd-64ba-4a64-bcf8-88938d02228e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 입력에 대한 반의어를 입력하세요.\n",
      "\n",
      "입력: 높은\n",
      "출력: 낮은\n",
      "\n",
      "입력: 재미있는\n",
      "출력: 지루한\n",
      "\n",
      "입력: 밝은\n",
      "출력: 어두운\n",
      "\n",
      "입력: 큰\n",
      "출력:\n"
     ]
    }
   ],
   "source": [
    "# SemanticSimilarityExampleSelector\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import FAISS, Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate\n",
    "\n",
    "examples = [\n",
    "            {'input': '밝은', 'output': '어두운'},\n",
    "            {'input': '재미있는', 'output': '지루한'},\n",
    "            {'input': '활기찬', 'output': '무기려한'},\n",
    "            {'input': '높은', 'output': '낮은'},\n",
    "            {'input': '빠른', 'output': '느린'}\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "                                input_variables = ['input', 'output'],\n",
    "                                template = '입력: {input}\\n출력: {output}'\n",
    ")\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "                                                                    examples = examples,\n",
    "                                                                    embeddings = OpenAIEmbeddings(),\n",
    "                                                                    vectorstore_cls = FAISS,\n",
    "                                                                    k = 3\n",
    ")\n",
    "\n",
    "prompt_from_string_examples = FewShotPromptTemplate(\n",
    "                                                    example_selector = example_selector,\n",
    "                                                    example_prompt = example_prompt,\n",
    "                                                    prefix = '모든 입력에 대한 반의어를 입력하세요.',\n",
    "                                                    suffix = '입력: {adjective}\\n출력:',\n",
    "                                                    input_variables = ['adjective'],\n",
    "                                                    example_separator = '\\n\\n'\n",
    ")\n",
    "\n",
    "print(prompt_from_string_examples.format(adjective = '큰'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13168058-9e1b-40bb-b528-c8f938663465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import MaxMarginalRelevanceExampleSelector\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate\n",
    "\n",
    "examples = [\n",
    "            {'input': '밝은', 'output': '어두운'},\n",
    "            {'input': '재미있는', 'output': '지루한'},\n",
    "            {'input': '활기찬', 'output': '무기력한'},\n",
    "            {'input': '높은', 'output': '낮은'},\n",
    "            {'input': '빠른', 'output': '느린'}\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "                                input_variables = ['input', 'output'],\n",
    "                                template = '입력: {input}\\n출력: {output}'\n",
    ")\n",
    "\n",
    "example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n",
    "                                                                        examples = examples,\n",
    "                                                                        embeddings = OpenAIEmbeddings(),\n",
    "                                                                        vectorstore_cls = FAISS,\n",
    "                                                                        k = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3897e7d-09f9-4b42-9282-c1ae064eb58d",
   "metadata": {},
   "source": [
    "# Chain 체인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3bb824-bcbc-44b1-84e4-97fa5a87768f",
   "metadata": {},
   "source": [
    "여러 개의 LLM이나 프롬프트의 입출력을 연결할 수 있는 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a41799d-2984-46ae-9ab1-e812bb4661cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = '''Q: {question}\n",
    "A:'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "                        input_variables = ['question'],\n",
    "                        template = template\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "                        llm = OpenAI(temperature = 0), # OpenAI를 통해 질문에 대한 답변 생성\n",
    "                        prompt = prompt,\n",
    "                        verbose = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "933d5f65-d15a-4d36-86bf-1fa1b18fed30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mQ: 기타를 잘 치는 방법은?\n",
      "A:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " 기타를 잘 치는 방법은 다음과 같습니다.\n",
      "\n",
      "1. 손가락 위치를 잘 잡아야 합니다. 기타는 손가락의 위치에 따라 소리가 달라집니다.\n",
      "\n",
      "2. 손가락을 잘 움직여야 합니다. 손가락을 잘 움직이면 소리가 더 잘 납니다.\n",
      "\n",
      "3. 손가락의 압력\n"
     ]
    }
   ],
   "source": [
    "question = '기타를 잘 치는 방법은?'\n",
    "print(llm_chain.predict(question = question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bd0cc5c-af15-42c4-9d7a-a83a2335f83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m고양이를 주제로 시를 작성해 주세요.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "나는 고양이를 사랑해\n",
      "내 마음은 반드시 그것을 따라\n",
      "고양이는 내 친구로 늘 나를 따라다니네\n",
      "\n",
      "고양이는 내 손길을 기다리며\n",
      "내 마음을 알고 있는 것 같아\n",
      "그것의 눈은 내 마음을 밝히는 빛이네\n",
      "\n",
      "고양이는 내 손길을 사랑해\n",
      "내\n"
     ]
    }
   ],
   "source": [
    "template = '{subject}를 주제로 {target}를 작성해 주세요.'\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "                        template = template,\n",
    "                        input_variables = ['subject', 'target']\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "                        llm = OpenAI(temperature = 0),\n",
    "                        prompt = prompt,\n",
    "                        verbose = True\n",
    ")\n",
    "\n",
    "print(llm_chain.predict(subject = '고양이', target = '시'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "248e3a47-0bcb-4ebd-b71b-a2608563868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Sequential Chain - 하나의 입출력\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "template = '''당신은 극작가입니다. 연극 제목이 주어졌을 때, 그 줄거리를 작성하는 것이 당신의 임무입니다.\n",
    "\n",
    "제목: {title}\n",
    "시놉시스:'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "                        input_variables = ['title'],\n",
    "                        template = template\n",
    ")\n",
    "\n",
    "chain1 = LLMChain(\n",
    "                    llm = OpenAI(temperature = 0),\n",
    "                    prompt = prompt\n",
    ")\n",
    "\n",
    "templte = '''당신은 연극 평론가입니다. 연극의 시놉시스가 주어지면 그 리뷰를 작성하는 것이 당신의 임무입니다.\n",
    "\n",
    "시놉시스:\n",
    "{synopsis}\n",
    "리뷰:'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "                        input_variables = ['synopsis'],\n",
    "                        template = template\n",
    ")\n",
    "\n",
    "chain2 = LLMChain(\n",
    "                    llm = OpenAI(temperature = 0),\n",
    "                    prompt = prompt\n",
    ")\n",
    "\n",
    "overall_chain = SimpleSequentialChain(\n",
    "                                        chains = [chain1, chain2],\n",
    "                                        verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c6eeed9d-33e1-4af4-8910-d4bbec2ddc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m \n",
      "\n",
      "서울 랩소디는 서울의 밤을 배경으로 하는 이야기입니다. 주인공은 서울의 밤을 사랑하는 랩소디로, 그녀는 밤에만 살고 싶어합니다. 그녀는 밤에만 살고 싶어하는 꿈을 가지고 있습니다. 그녀는 서울의 밤을 사랑하는 랩소\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "서울 랩소디는 서울의 밤을 배경으로 하는 이야기입니다. 주인공은 서울의 밤을 사랑하는 랩소디로, 그녀는 밤에만 살고 싶어합니다. 그녀는 밤에만 살고 싶어하는 꿈을 가지고 있습니다. 그녀는 서울의 밤을 사랑하는 랩소\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "서울 랩소디는 서울의 밤을 배경으로 하는 이야기입니다. 주인공은 서울의 밤을 사랑하는 랩소디로, 그녀는 밤에만 살고 싶어합니다. 그녀는 밤에만 살고 싶어하는 꿈을 가지고 있습니다. 그녀는 서울의 밤을 사랑하는 랩소\n"
     ]
    }
   ],
   "source": [
    "print(overall_chain.run('서울 랩소디'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "07a941e4-d7b9-45fa-a242-bf1eea59cf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Chain - 여러 개의 입출력\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "template = '''당신은 극작가입니다. 극의 제목과 시대적 배경이 주어졌을 때, 그 줄거리를 작성하는 것이 당신의 임무입니다.\n",
    "\n",
    "제목: {title}\n",
    "시대: {era}\n",
    "시놉시스:'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "                        input_variables = ['title', 'era'],\n",
    "                        template = template\n",
    ")\n",
    "\n",
    "chain1 = LLMChain(\n",
    "                    llm = OpenAI(temperature = 0),\n",
    "                    prompt = prompt,\n",
    "                    output_key = 'synopsis'\n",
    ")\n",
    "\n",
    "template = '''당신은 연극 평론가입니다. 연극의 시놉시스가 주어지면 그 리뷰를 작성하는 것이 당신의 임무입니다.\n",
    "\n",
    "시놉시스:\n",
    "{synopsis}\n",
    "리뷰:'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "                        input_variables = ['synopsis'],\n",
    "                        template = template\n",
    ")\n",
    "\n",
    "chain2 = LLMChain(\n",
    "                    llm = OpenAI(temperature = 0),\n",
    "                    prompt = prompt,\n",
    "                    output_key = 'review'\n",
    ")\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "                                chains = [chain1, chain2],\n",
    "                                input_variables = ['title', 'era'],\n",
    "                                output_variables = ['synopsis', 'review'],\n",
    "                                verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "827e4a97-2a0d-4998-bac7-cfccce952096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'title': '서울 랩소디', 'era': '100년 후의 미래', 'synopsis': ' \\n\\n미래의 서울은 새로운 랩소디로 바뀌었습니다. 인간들은 인공지능과 기술의 발전으로 더 나은 삶을 위해 노력하고 있습니다. 그러나 이 모든 것은 인간들의 자유를 제한하고 있습니다. \\n\\n주인공은 이러한 상황에 대해 반발하는', 'review': '\\n\\n이 연극은 미래의 서울을 배경으로 하는 인공지능과 기술의 발전이 인간들의 자유를 제한하는 것에 대한 반발을 담고 있습니다. 주인공은 이러한 상황에 대해 반발하고 자신의 자유를 위해 노력합니다. 이 연극은 인간의 자유를 위해'}\n"
     ]
    }
   ],
   "source": [
    "print(overall_chain({'title': '서울 랩소디', 'era': '100년 후의 미래'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9890f7-f8ef-48c1-b108-ea307b8d51e3",
   "metadata": {},
   "source": [
    "### 인덱스 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ae53f6aa-c847-4108-98fe-5e450edd8108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "with open('akazukin_all.txt') as f:\n",
    "    test_all = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "47468285-302a-4d32-baee-d2b1c436c882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "제목: '전뇌 빨간 : 299\n",
      "제2장: 울프 코퍼 : 162\n",
      "제3장: 배신과 재 : 273\n",
      "제4장: 울프 코퍼 : 206\n",
      "제5장: 결전의 순 : 294\n",
      "제7장: 새로운 시 : 195\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "                                        separator = '\\n\\n',\n",
    "                                        chunk_size = 300, # 청크의 최대 문자 수\n",
    "                                        chunk_overlap = 20 # 겹치는 최대 문자 수\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_text(test_all)\n",
    "\n",
    "print(len(texts))\n",
    "for text in texts:\n",
    "    print(text[:10], ':', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0fd55d5a-73c6-47f2-98e0-ca622bff75d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "docsearch = FAISS.from_texts(\n",
    "                                texts = texts,\n",
    "                                embedding = OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "                                        llm = OpenAI(temperature = 0), # LLM\n",
    "                                        chain_type = 'stuff', # 체인 종류\n",
    "                                        retriever = docsearch.as_retriever() # 리트리버\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c153cdd7-97d2-4d11-9e20-fb6332e052b3",
   "metadata": {},
   "source": [
    "체인의 종류\n",
    "- stuff: 모든 데이터를 프롬프트에 담아 전달하는 방식\n",
    "- map_reduce: 청크 단위로 분할 후 프롬프트에 전달 -> 결과를 결합\n",
    "- refine: 청크 단위로 분할 후, 순차적으로 프롬프트에 전달 -> 결과 생성 -> 결과를 다시 프롬프트에 전달\n",
    "- map_rerank: 청크 단위로 분할 후 프롬프트에 전달 -> 결과에 대한 점수 표기 -> 높은 점수 기준으로 응답 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "44c5dbdf-67d8-42c7-b81f-73fe8871b524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 료\n"
     ]
    }
   ],
   "source": [
    "print(qa_chain.run('미코의 소꿉친구 이름은?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0ac92c0f-0065-4464-a9ec-f9e45fdfdb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RetrievalQAWithSourcesChain\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "meta_data = [\n",
    "                {'source': '1장'},\n",
    "                {'source': '2장'},\n",
    "                {'source': '3장'},\n",
    "                {'source': '4장'},\n",
    "                {'source': '5~6장'},\n",
    "                {'source': '7장'}\n",
    "]\n",
    "\n",
    "docsearch = FAISS.from_texts(\n",
    "                                texts = texts,\n",
    "                                embedding = OpenAIEmbeddings(),\n",
    "                                metadatas = meta_data\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "                                                        llm = OpenAI(temperature = 0),\n",
    "                                                        chain_type = 'stuff',\n",
    "                                                        retriever = docsearch.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "02a7241f-e205-4b19-80fc-e70f4950671f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': '미코의 친구 이름은?', 'answer': \" Miku's friend's name is not given.\\n\", 'sources': '2장, 7장, 5~6장, 3장'}\n"
     ]
    }
   ],
   "source": [
    "print(qa_chain({'question': '미코의 친구 이름은?'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a1b1cc51-1d02-47dc-b4b8-6cb5577581df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SummarizeChain - 요약을 위한 체인\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "docs = [Document(page_content = t) for t in texts]\n",
    "\n",
    "chain = load_summarize_chain(\n",
    "                                llm = OpenAI(temperature = 0),\n",
    "                                chain_type = 'map_reduce'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "50f23396-4bf6-4d20-b0c9-03e990294ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n미코는 불법 데이터 카우리아를 운반하는 배달원으로 일하는데, 어느 날 거대 기업 '울프 코퍼레이션'의 시민에 대한 악랄한 지배를 폭로하는 정보가 담긴 데이터를 운반하는 임무를 맡게 된다. 미코와 료는 울프 코퍼레\""
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e34859-582d-423c-b428-94cb8649740b",
   "metadata": {},
   "source": [
    "### 유틸리티 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e495a08e-2ff0-488b-b52e-1e2c0bc62a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new PALChain chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mdef solution():\n",
      "    \"\"\"Jane is raising three times as many pets as Alice. If Alice is raising two pets, how many pets are both of them raising in total?\"\"\"\n",
      "    alice_pets = 2\n",
      "    jane_pets_factor = 3\n",
      "    jane_pets = alice_pets * jane_pets_factor\n",
      "    total_pets = alice_pets + jane_pets\n",
      "    result = total_pets\n",
      "    return result\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# PALChain\n",
    "from langchain_experimental.pal_chain.math_prompt import MATH_PROMPT\n",
    "\n",
    "from langchain_experimental import pal_chain\n",
    "\n",
    "# 수학적 계산\n",
    "from langchain import OpenAI\n",
    "\n",
    "pal_chain = pal_chain.PALChain.from_math_prompt(\n",
    "                                                llm = OpenAI(),\n",
    "                                                verbose = True\n",
    ")\n",
    "\n",
    "question = '제인은 앨리스가 키우는 반려동물의 3배가 되는 반려동물을 키우고 있다. 앨리스가 2마리의 반려동물을 키우고 있다면 두 사람이 키우고 있는 반려동물의 총 마리 수는?'\n",
    "print(pal_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "87c03e67-64a5-4241-8baa-2a80811a8b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAIModerationChain - 컨텐츠 정책 준수 여부를 판단하는 체인\n",
    "from langchain.chains import OpenAIModerationChain\n",
    "\n",
    "chain = OpenAIModerationChain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2bc18d34-9745-4d44-ae02-f5eb9c29200d",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.Moderation, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/langchain/chains/base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    311\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    312\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    314\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/langchain/chains/base.py:304\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    297\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    298\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    299\u001b[0m     inputs,\n\u001b[1;32m    300\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    301\u001b[0m )\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 304\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    307\u001b[0m     )\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/langchain/chains/moderation.py:93\u001b[0m, in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     90\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m     91\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     92\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m---> 93\u001b[0m     text \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key]\n\u001b[1;32m     94\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(text)\n\u001b[1;32m     95\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_moderate(text, results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/openai/_utils/_proxy.py:22\u001b[0m, in \u001b[0;36mLazyProxy.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_proxied__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, attr)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/openai/_utils/_proxy.py:43\u001b[0m, in \u001b[0;36mLazyProxy.__get_proxied__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__get_proxied__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_cache:\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     proxied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__proxied\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m proxied \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/openai/lib/_old_api.py:33\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__load__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__load__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.Moderation, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "chain.__call__('hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5f2513-f803-4295-88a2-a4561edbce53",
   "metadata": {},
   "source": [
    "# Agent 에이전트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d8e9dc93-0c58-42cf-8a57-e0b4afda244e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-search-results\n",
      "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from google-search-results) (2.28.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->google-search-results) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->google-search-results) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->google-search-results) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->google-search-results) (2.1.1)\n",
      "Building wheels for collected packages: google-search-results\n",
      "  Building wheel for google-search-results (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32002 sha256=1bcc4e60a99557212bbeae25e90af2e9fbfec69b0f7ee9ec17716b68c1f2aa70\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/68/8e/73/744b7d9d7ac618849d93081a20e1c0deccd2aef90901c9f5a9\n",
      "Successfully built google-search-results\n",
      "Installing collected packages: google-search-results\n",
      "Successfully installed google-search-results-2.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install google-search-results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
